import os
import re
import time
import random
import requests
import jieba
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt
from bs4 import BeautifulSoup
from wordcloud import WordCloud
from collections import Counter
from concurrent.futures import ThreadPoolExecutor
from fake_useragent import UserAgent
from snownlp import SnowNLP

# ================ å…¨å±€å­—ä½“åˆå§‹åŒ– ================
# å¿…é¡»åœ¨å…¶ä»–å¯¼å…¥ä¹‹å‰è®¾ç½®
mpl.use('Agg')  # è§£å†³æ— GUIç¯å¢ƒé—®é¢˜

# é…ç½®ç³»ç»Ÿå­—ä½“ï¼ˆWindows/Mac/Linuxè‡ªåŠ¨é€‚é…ï¼‰
try:
    if os.name == 'nt':  # Windowsç³»ç»Ÿ
        mpl.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
    else:  # Mac/Linuxç³»ç»Ÿ
        mpl.rcParams['font.sans-serif'] = ['PingFang HK', 'Noto Sans CJK SC', 'WenQuanYi Zen Hei']

    mpl.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
    plt.rcParams['font.size'] = 12  # å…¨å±€å­—ä½“å¤§å°

    # éªŒè¯å­—ä½“é…ç½®
    test_fig, test_ax = plt.subplots()
    test_ax.set_title("ä¸­æ–‡æµ‹è¯•")
    test_fig.savefig(os.path.join(os.getcwd(), 'font_test.png'))
    plt.close(test_fig)
    print("âœ… ç³»ç»Ÿå­—ä½“é…ç½®éªŒè¯é€šè¿‡")
except Exception as e:
    print("âŒ å­—ä½“é…ç½®å¤±è´¥:", str(e))
    print("è¯·æ‰§è¡Œä»¥ä¸‹è§£å†³æ–¹æ¡ˆï¼š")
    print("1. Windowsç³»ç»Ÿï¼šå®‰è£…[å¾®è½¯é›…é»‘](https://learn.microsoft.com/zh-cn/typography/font-list/microsoft-yahei)")
    print("2. Mac/Linuxç³»ç»Ÿï¼šæ‰§è¡Œå®‰è£…å‘½ä»¤ï¼šsudo apt install fonts-noto-cjk")
    exit(1)

# ================== é…ç½® ==================
CONFIG = {
    # å¤šç”µå½±é…ç½®ï¼ˆè±†ç“£ID: ç”µå½±åç§°ï¼‰
    'movies': {
        '30181250': 'å°ç¥ç¬¬äºŒéƒ¨ï¼šæˆ˜ç«è¥¿å²',
        '36282639': 'å”æ¢1900',
        '34780991': 'å“ªå’ä¹‹é­”ç«¥é—¹æµ·',
        '36289423': 'å°„é›•è‹±é›„ä¼ ï¼šä¾ ä¹‹å¤§è€…',
        '35295960': 'è›Ÿé¾™è¡ŒåŠ¨',
        '36970301':'ç†Šå‡ºæ²¡Â·é‡å¯æœªæ¥',
    },
    'output_dir': './reports',  # è¾“å‡ºç›®å½•
    'page_limit': 5,  # æ¯éƒ¨ç”µå½±æŠ“å–é¡µæ•°
    'max_workers': 5,  # å¹¶å‘çº¿ç¨‹æ•°
    'proxy_pool': [  # ä»£ç†IPæ± 
        # 'http://ip1:port',
        # 'http://ip2:port'
    ],
    'filterRoleNames':True,
    'font_path':'./font/NotoSansCJKMedium.otf',  # å­—ä½“
    'filterText':'./filterText.txt',
    'stopwords': './stopwords.txt',  # åœç”¨è¯æ–‡ä»¶è·¯å¾„
    'sentiment_threshold': (0.4, 0.6)  # æƒ…æ„Ÿé˜ˆå€¼(è´Ÿé¢, ä¸­æ€§)
}


# =============================================

class MovieAnalyzer:
    def __init__(self):
        self.ua = UserAgent()
        os.makedirs(CONFIG['output_dir'], exist_ok=True)

        # åˆå§‹åŒ–åˆ†è¯å™¨
        jieba.load_userdict('./userdict.txt')  # è‡ªå®šä¹‰è¯å…¸

        # åŠ è½½åœç”¨è¯
        with open(CONFIG['stopwords'], 'r', encoding='utf-8') as f:
            self.stopwords = set(f.read().splitlines())

    def get_headers(self):
        """ç”ŸæˆåŠ¨æ€è¯·æ±‚å¤´"""
        return {
            'User-Agent': self.ua.random,
            'Referer': 'https://movie.douban.com/'
        }

    def get_proxy(self):
        """éšæœºè·å–ä»£ç†IP"""
        return random.choice(CONFIG['proxy_pool']) if CONFIG['proxy_pool'] else None

    def fetch_data(self, movie_id):
        """å¤šçº¿ç¨‹å®‰å…¨çš„æ•°æ®æŠ“å–"""
        all_comments = []
        character_blacklist = []

        try:
            # è·å–æ¼”å‘˜è¡¨
            url = f'https://movie.douban.com/subject/{movie_id}/celebrities'
            resp = requests.get(url, headers=self.get_headers(),
                                proxies={'http': self.get_proxy()}, timeout=15)
            soup = BeautifulSoup(resp.text, 'html.parser')
            if CONFIG['filterRoleNames']:
                character_blacklist = [li.find('span', class_='name').text
                                   for li in soup.select('li.celebrity')[:8]]


            # è·å–çŸ­è¯„
            with ThreadPoolExecutor(max_workers=3) as executor:
                futures = []
                for page in range(CONFIG['page_limit']):
                    futures.append(
                        executor.submit(self._fetch_page_comments,
                                        movie_id, page)
                    )
                    time.sleep(random.uniform(0.5, 1.5))
                for future in futures:
                    all_comments.extend(future.result())

        except Exception as e:
            print(f'ç”µå½±{movie_id}æ•°æ®è·å–å¼‚å¸¸: {str(e)}')

        return {
            'comments': all_comments,
            'characters': character_blacklist
        }

    def _fetch_page_comments(self, movie_id, page):
        """å•é¡µè¯„è®ºæŠ“å–"""
        try:
            url = f'https://movie.douban.com/subject/{movie_id}/comments?start={page * 20}'
            print('url:',url)
            resp = requests.get(url, headers=self.get_headers(),
                                proxies={'http': self.get_proxy()}, timeout=10)
            soup = BeautifulSoup(resp.text, 'html.parser')
            comments = [self._clean_text(span.get_text())
                        for span in soup.select('span.short')]
            time.sleep(random.uniform(1, 3))
            print('è·å–ç¬¬',page + 1,'é¡µè¯„è®ºæˆåŠŸ')
            return comments
        except Exception as e:
            print('è·å–ç¬¬',page + 1 ,'é¡µè¯„è®ºå‡ºé”™:', e)
            return []

    def _clean_text(self, text):
        """é«˜çº§æ–‡æœ¬æ¸…æ´—"""
        text = re.sub(r'<[^>]+>', '', text)  # HTMLæ ‡ç­¾
        text = re.sub(r'@\w+\s?', '', text)  # å»é™¤@æåŠ
        text = re.sub(r'ã€.*?ã€‘', '', text)  # å»é™¤æ‹¬å·å†…å®¹
        text = re.sub(r'[^\w\u4e00-\u9fff]', ' ', text)  # ä¿ç•™ä¸­æ–‡å’ŒåŸºæœ¬å­—ç¬¦
        return text.strip()

    def analyze_movie(self, movie_id, movie_name):
        """æ ¸å¿ƒåˆ†ææµç¨‹"""
        print(f'ğŸ¬ æ­£åœ¨åˆ†æã€Š{movie_name}ã€‹...')
        data = self.fetch_data(movie_id)

        if not data['comments']:
            print(f'âš ï¸ ã€Š{movie_name}ã€‹æ— æœ‰æ•ˆè¯„è®º')
            return

        # æƒ…æ„Ÿåˆ†æä¸æ–‡æœ¬å¤„ç†
        sentiment_results = []
        words = []
        characters_arr = []
        filter_text = []
        for ch in data['characters']:
            split_string = ch.split()
            characters_arr.extend(split_string)
        with open(CONFIG['filterText'], 'r', encoding='utf-8') as f:
            filter_text = set(f.read().splitlines())
        blacklist = set(characters_arr + list(filter_text))
        print("blacklist", blacklist)

        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = []
            for comment in data['comments']:
                futures.append(executor.submit(self._process_comment, comment, blacklist))

            for future in futures:
                result = future.result()
                if result:
                    words.extend(result['words'])
                    sentiment_results.append(result['sentiment'])

        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        self._generate_wordcloud(words, movie_name)
        self._generate_sentiment_chart(sentiment_results, movie_name)
        self._generate_full_report(words, sentiment_results, movie_name)

    def _process_comment(self, comment, blacklist):
        """å¤„ç†å•æ¡è¯„è®ºï¼ˆåŒ…å«æƒ…æ„Ÿåˆ†æï¼‰"""
        try:
            # æƒ…æ„Ÿåˆ†æ
            s = SnowNLP(comment)
            sentiment = s.sentiments

            # æ–‡æœ¬å¤„ç†
            seg = jieba.lcut(comment)
            filtered_words = [w for w in seg if len(w) > 1
                              and w not in self.stopwords
                              and w not in blacklist]

            return {
                'words': filtered_words,
                'sentiment': sentiment
            }
        except:
            return None

    def _generate_wordcloud(self, words, movie_name):
        """ç”Ÿæˆé«˜çº§è¯äº‘"""
        freq = Counter(words)

        wc = WordCloud(
            font_path=CONFIG['font_path'],
            width=1600,
            height=1200,
            background_color='white',
            colormap='tab20',
            max_words=200,
            contour_width=1,
            contour_color='steelblue'
        ).generate_from_frequencies(freq)

        plt.figure(figsize=(20, 15))
        plt.imshow(wc, interpolation='bilinear')
        plt.axis('off')
        plt.savefig(os.path.join(CONFIG['output_dir'],
                                 f'{movie_name}_è¯äº‘.png'),
                    bbox_inches='tight', dpi=300)
        plt.close()

    def _generate_sentiment_chart(self, sentiments, movie_name):
        """ç”Ÿæˆæƒ…æ„Ÿåˆ†å¸ƒé¥¼å›¾"""
        low, high = CONFIG['sentiment_threshold']
        counts = {
            'è´Ÿé¢': sum(1 for s in sentiments if s < low),
            'ä¸­æ€§': sum(1 for s in sentiments if low <= s <= high),
            'æ­£é¢': sum(1 for s in sentiments if s > high)
        }

        plt.figure(figsize=(10, 10))
        plt.pie(
            counts.values(),
            labels=counts.keys(),
            autopct='%1.1f%%',
            colors=['#ff9999', '#66b3ff', '#99ff99'],
            startangle=90
        )
        plt.title(f'ã€Š{movie_name}ã€‹è¯„è®ºæƒ…æ„Ÿåˆ†å¸ƒ', fontsize=14)
        plt.savefig(os.path.join(CONFIG['output_dir'],
                                 f'{movie_name}_æƒ…æ„Ÿåˆ†å¸ƒ.png'),
                    bbox_inches='tight', dpi=150)
        plt.close()

    def _generate_full_report(self, words, sentiments, movie_name):
        """ç”Ÿæˆå®Œæ•´åˆ†ææŠ¥å‘Š"""
        # æƒ…æ„Ÿæ•°æ®
        df_sentiment = pd.DataFrame({
            'æƒ…æ„Ÿå¾—åˆ†': sentiments,
            'æƒ…æ„Ÿåˆ†ç±»': ['æ­£é¢' if s > CONFIG['sentiment_threshold'][1] else
                         'ä¸­æ€§' if s >= CONFIG['sentiment_threshold'][0] else
                         'è´Ÿé¢' for s in sentiments]
        })

        # å…³é”®è¯æ•°æ®
        freq = Counter(words)
        df_keywords = pd.DataFrame(freq.most_common(50),
                                   columns=['å…³é”®è¯', 'é¢‘æ¬¡'])

        # ä¿å­˜Excel
        with pd.ExcelWriter(os.path.join(CONFIG['output_dir'],
                                         f'{movie_name}_åˆ†ææŠ¥å‘Š.xlsx')) as writer:
            df_sentiment.to_excel(writer, sheet_name='æƒ…æ„Ÿåˆ†æ', index=False)
            df_keywords.to_excel(writer, sheet_name='å…³é”®è¯åˆ†æ', index=False)

            # æ·»åŠ ç»Ÿè®¡æ•°æ®
            stats = pd.DataFrame({
                'æŒ‡æ ‡': ['æ€»è¯„è®ºæ•°', 'å¹³å‡æƒ…æ„Ÿå¾—åˆ†', 'æ­£é¢ç‡', 'è´Ÿé¢ç‡'],
                'æ•°å€¼': [
                    len(sentiments),
                    sum(sentiments) / len(sentiments),
                    sum(1 for s in sentiments if s > CONFIG['sentiment_threshold'][1]) / len(sentiments),
                    sum(1 for s in sentiments if s < CONFIG['sentiment_threshold'][0]) / len(sentiments)
                ]
            })
            stats.to_excel(writer, sheet_name='ç»Ÿè®¡æ¦‚è§ˆ', index=False)

        # ç”Ÿæˆè¶‹åŠ¿å›¾
        plt.figure(figsize=(12, 6))
        df_keywords.head(15).plot.bar(x='å…³é”®è¯', y='é¢‘æ¬¡', legend=False)
        plt.title(f'ã€Š{movie_name}ã€‹é«˜é¢‘å…³é”®è¯TOP15')
        plt.tight_layout()
        plt.savefig(os.path.join(CONFIG['output_dir'],
                                 f'{movie_name}_è¶‹åŠ¿å›¾.png'), dpi=150)
        plt.close()


if __name__ == '__main__':
    analyzer = MovieAnalyzer()

    print('=' * 50)
    print('ğŸ‰ è±†ç“£ç”µå½±åˆ†æç³»ç»Ÿå¯åŠ¨ï¼ˆæƒ…æ„Ÿåˆ†æç‰ˆï¼‰')
    print('=' * 50)

    # æ‰¹é‡å¤„ç†ç”µå½±
    for movie_id, movie_name in CONFIG['movies'].items():
        analyzer.analyze_movie(movie_id, movie_name)

    print('\nâœ… åˆ†æå®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³reportsç›®å½•')